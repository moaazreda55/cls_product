{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7b1589c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\project\\cls_product\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b6ea07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "food = load_dataset(\"food101\",split=\"train[:5000]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812176f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "food_split = food.train_test_split(0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ed54c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = food_split['train'].features['label'].names\n",
    "label2id, id2label = {}, {}\n",
    "for i , label in enumerate(labels):\n",
    "    label2id[label] = i,\n",
    "    id2label[id] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a675db9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, AutoModelForImageClassification, TrainingArguments, Trainer\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained(\"facebook/convnext-base-224\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b59b70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = processor.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04056863",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import RandomResizedCrop ,Compose ,Resize ,Normalize ,ToTensor\n",
    "transform = Compose([\n",
    "    RandomResizedCrop(size['shortest_edge']),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=processor.image_mean,std=processor.image_std)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ffc928",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def process_image(samples):\n",
    "    samples['pixel_values'] = [transform(img.convert('RGB')) for img in samples['image']]\n",
    "    del samples['image']\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f75e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "food_split_proc = food_split.with_transform(process_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b46677",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForImageClassification ,TrainingArguments ,Trainer,default_data_collator\n",
    "import evaluate\n",
    "import numpy as np\n",
    "data_collator = default_data_collator\n",
    "metric = evaluate.load('accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(predictions_and_labels):\n",
    "    predictions,labels = predictions_and_labels\n",
    "    predictions = np.argmax(predictions,axis=-1)\n",
    "    return metric.compute(predictions=predictions,references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55986b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForImageClassification.from_pretrained('google/vit-base-patch16-224-in21k',num_labels=len(labels),id2label=id2label,label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d1a062",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    # output_dir=\"my_model\",\n",
    "    output_dir=\"./results\",\n",
    "    report_to=[],\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    eval_strategy=\"epoch\",\n",
    "    weight_decay=0.01,\n",
    "    remove_unused_columns=False,\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=0.001,\n",
    "    num_train_epochs=7,\n",
    "    logging_steps=10,\n",
    "    push_to_hub=False,\n",
    "    metric_for_best_model=\"accuracy\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21c1922",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=food_split_proc['train'],\n",
    "    eval_dataset=food_split_proc['test'],\n",
    "    processing_class=processor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a0774f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
